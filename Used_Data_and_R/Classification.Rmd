---
title: "Classification"
author: "Sar"
date: "3/29/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# library statements 
# read in data

library(ISLR)
library(dplyr)
library(readr)
library(broom)
library(ggplot2)
library(splines)
library(tidymodels)
library(gridExtra)
library(maps)
tidymodels_prefer()

COVID_State <- read.csv("COVID - State - Daily.csv", na.strings = ".")

Employment_State <- read.csv("Employment - State - Daily.csv", na.strings = ".")

Mobility_State <- read.csv("Google Mobility - State - Daily.csv", na.strings = ".")
  
Spending_State <- read.csv("Affinity - State - Daily.csv", na.strings = ".")

regions <- read.csv("regions.csv")
fips <- state.fips
```


```{r}
# data cleaning

COVID_State$Date<-as.Date(with(COVID_State,paste(year,month,day,sep="-")),"%Y-%m-%d")

Employment_State$Date<-as.Date(with(Employment_State,paste(year,month,day,sep="-")),"%Y-%m-%d")

Mobility_State$Date<-as.Date(with(Mobility_State,paste(year,month,day,sep="-")),"%Y-%m-%d")

Spending_State$Date<-as.Date(with(Spending_State,paste(year,month,day,sep="-")),"%Y-%m-%d")

full_data <- merge(merge(merge(COVID_State, Employment_State, by=c("Date","statefips")), Mobility_State, by=c("Date","statefips")), Spending_State, by=c("Date","statefips"))

head(full_data)

full_data1 <- full_data %>%
  select(-year.x, -month.x, -day.x, - year.y, -month.y, -day.y, -year.x )

regions <- regions%>%
  inner_join(fips, by=c("State.Code"="abb"))

#created dataset with the fips code
full_cut <- full_data1 %>%
  filter(Date > "2020-04-13")%>%
  select(statefips, Date, gps_away_from_home, case_rate, hospitalized_rate, spend_remoteservices, spend_hcs, emp_incbelowmed)%>%
  left_join(regions, by=c("statefips"="fips"))

full_cut <- full_cut %>%
  select(statefips, Date, gps_away_from_home, case_rate, hospitalized_rate, spend_remoteservices, spend_hcs, emp_incbelowmed,State.Code, Region, Division)


```


```{r}
full_cut <- full_cut[,-1]
full_cut <- full_cut %>% na.omit() #there are 6 missing values in two variables
full_cut <- full_cut %>%
  mutate(Region = factor(Region)) %>% #make sure outcome is factor
  mutate(across(where(is.character), as.factor))
```


##  Bagging

### Model Specification

Recipes first this time...

```{r}
spade_rec <- recipe(Region ~ ., data = full_cut) %>%
  step_rm(State.Code, Division)%>%
  step_nzv(all_predictors()) %>%
  step_novel(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) 

# Find out the Total Number of Predictors
(spade_rec %>% prep(full_cut) %>% juice() %>% ncol()) - 1 #subtract 1 for outcome
```

```{r}
# Bagging Model Spec
bag_spec <- rand_forest() %>%
  set_engine(engine = 'ranger') %>% 
  set_args(mtry = 61, # Specify total number of predictors (use prep code above)
           trees = 50, # Number of trees/bags (bootstrap aggregation)
           min_n = 2,# Min observations to split (want small to create larger trees)
           probability = FALSE) %>% # For now, we want hard predictions, not soft
  set_mode('classification') # change this for regression tree

bag_spec


region_bag_wf <- workflow() %>%
  add_model(bag_spec) %>%
  add_recipe(spade_rec)
```



### Fit Models

```{r}
set.seed(123) # Randomness in the bootstrap samples

region_bag_fit <- region_bag_wf %>%
  fit(data = full_cut)

region_bag_fit # check out OOB prediction error (accuracy = 1 - OOB prediction error)
```


### Evaluate Models

To  calculate OOB metrics, we need to get the OOB predictions from the fit model.

```{r}
region_bag_OOB_output <- tibble(
  .pred_class = region_bag_fit %>% extract_fit_engine() %>% pluck('predictions'),
  Region = full_cut %>% pull(Region))

bag_metrics <- metric_set(sens, yardstick::spec, accuracy)

region_bag_OOB_output %>% 
  bag_metrics(truth = Region, estimate = .pred_class)
```

To estimate AUC of ROC curve based on OOB predictions, we'll need to refit the model to get the predicted probabilities. 

```{r}
set.seed(123) # to get the same bootstrap samples, use same seed

region_bag_fit2 <- region_bag_wf %>%
  update_model(bag_spec %>% set_args(probability = TRUE)) %>% # Now, we want soft (probability) predictions
  fit(data = full_cut)

region_bag_fit2
```

```{r}
region_bag_OOB_output2 <- bind_cols(
  region_bag_fit2 %>% extract_fit_engine() %>% pluck('predictions') %>% as_tibble(),
  full_cut %>% select(Region))

region_bag_OOB_output2 %>% 
  roc_curve(Region, c(South, West, Midwest, Northeast), event_level = "second") %>% autoplot()

region_bag_OOB_output2 %>% 
  roc_auc(Region, c(South, West, Midwest, Northeast), event_level = "second") #Area under Curve
```


##  Random Forest

### Model Specification


```{r}
rf_spec <- rand_forest() %>%
  set_engine(engine = 'ranger') %>% 
  set_args(mtry = NULL, # size of random subset of variables; default is floor(sqrt(ncol(x)))
           trees = 50, # Number of bags
           min_n = 2,
           probability = FALSE, # want hard predictions first
           importance = 'impurity') %>% 
  set_mode('classification') # change this for regression tree

rf_spec


region_rf_wf <- workflow() %>%
  add_model(rf_spec) %>%
  add_recipe(spade_rec)
```



### Fit Models

```{r}
set.seed(123)
region_rf_fit <- region_rf_wf %>%
  fit(data = full_cut)

region_rf_fit # check out OOB prediction error (accuracy = 1 - OOB prediction error)
```


### Evaluate Models

To  calculate OOB metrics, we need to get the OOB predictions from the fit model.

```{r}
region_rf_OOB_output <- tibble(
  .pred_class = region_rf_fit %>% extract_fit_engine() %>% pluck('predictions'),
  Region = full_cut %>% pull(Region))

bag_metrics <- metric_set(sens, yardstick::spec, accuracy)

region_rf_OOB_output %>% 
  bag_metrics(truth = Region, estimate = .pred_class)
```

To estimate AUC of ROC curve using OOB predictions, we'll need to refit the model to get the predicted probabilities. 

```{r}
set.seed(123) #to get the same bootstrap samples, use same seed
region_rf_fit2 <- region_rf_wf %>%
  update_model(rf_spec %>% set_args(probability = TRUE)) %>%
  fit(data = full_cut)

region_rf_fit2
```

```{r}
region_rf_OOB_output2 <- bind_cols(
  region_rf_fit2 %>% extract_fit_engine() %>% pluck('predictions') %>% as_tibble(),
  full_cut%>% select(Region))

region_rf_OOB_output2 %>% 
  roc_curve(Region, c(South, West, Midwest, Northeast), event_level = "second") %>% autoplot()

region_rf_OOB_output2 %>% 
  roc_auc(Region, c(South, West, Midwest, Northeast), event_level = "second") #Area under Curve
```

### Variable Importance


```{r}
library(vip) #install.packages('vip')

region_rf_fit %>% extract_fit_engine() %>% vip() #based on impurity

region_rf_wf %>% #based on permutation
  update_model(rf_spec %>% set_args(importance = "permutation")) %>%
  fit(data = full_cut) %>% extract_fit_engine() %>% vip()
```

### Hierarchical Clustering

```{r}
set.seed(253)
full_cut <- full_cut %>%
    slice_sample(n = 50)

# Select the variables to be used in clustering
full_cut_sub <- full_cut %>%
    select(gps_away_from_home, case_rate, hospitalized_rate, spend_remoteservices, spend_hcs, emp_incbelowmed)

# Summary statistics for the variables
summary(full_cut_sub)

# Compute a distance matrix on the scaled data
dist_mat_scaled <- dist(scale(full_cut_sub))

# The (scaled) distance matrix is the input to hclust()
# The method argument indicates the linkage type
hc_complete <- hclust(dist_mat_scaled, method = "complete")
hc_single <- hclust(dist_mat_scaled, method = "single")
hc_average <- hclust(dist_mat_scaled, method = "average")
hc_centroid <- hclust(dist_mat_scaled, method = "centroid")

# Plot dendrograms
plot(hc_complete)
plot(hc_single)
plot(hc_average)
plot(hc_centroid)

#plot with labels
plot(hc_complete, labels = full_cut$Region)

#scatterplot with colors
full_cut <- full_cut %>%
    mutate(
        hclust_height3 = factor(cutree(hc_complete, h = 3)), # Cut at height (h) 3
        hclust_num6 = factor(cutree(hc_complete, k = 6)) # Cut into 6 clusters (k)
    )


ggplot(full_cut, aes(x=gps_away_from_home, y=case_rate, color=hclust_height3))+
    geom_point()+
    theme_bw()

```


