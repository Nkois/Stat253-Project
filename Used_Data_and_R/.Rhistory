region_rf_OOB_output2 <- bind_cols(
region_rf_fit2 %>% extract_fit_engine() %>% pluck('predictions') %>% as_tibble(),
full_cut%>% select(Region))
region_rf_OOB_output3 <- bind_cols(
.pred_class = region_rf_fit2 %>% extract_fit_engine() %>% pluck('predictions'),
Region = full_cut %>% pull(Region))
region_rf_OOB_output2 %>%
roc_curve(Region, c(South, West, Midwest, Northeast), event_level = "second") %>% autoplot()
region_rf_OOB_output2 %>%
roc_auc(Region, c(South, West, Midwest, Northeast), event_level = "second") #Area under Curve
region_rf_fit %>% extract_fit_engine() %>% vip() #based on impurity
library(ISLR)
library(dplyr)
library(readr)
library(broom)
library(ggplot2)
library(splines)
library(tidymodels)
library(gridExtra)
library(maps)
library(caret)
library(vip) #install.packages('vip')
tidymodels_prefer()
region_rf_fit %>% extract_fit_engine() %>% vip() #based on impurity
region_rf_wf %>% #based on permutation
update_model(rf_spec %>% set_args(importance = "permutation")) %>%
fit(data = full_cut) %>% extract_fit_engine() %>% vip()
#rf_OOB_output(data_fit_mtry12,12, land %>% pull(class)) %>%
#    conf_mat(truth = class, estimate= .pred_class)
region_rf_OOB_output(region_rf_fit2, NULL, full_cut %>% pull(Region)) %>%
conf_mat(truth = Region, estimate = .pred_class)
# log_metrics <- metric_set(sens, yardstick::spec, accuracy) # these metrics are based on hard predictions
#sens: sensitivity = chance of correctly predicting second level, given second level (Yes)
#spec: specificity = chance of correctly predicting first level, given first level (No)
#accuracy: accuracy = chance of correctly predicting outcome
# region_rf_OOB_output3 %>%
#   log_metrics(estimate = .pred_class, truth = c(South, West, Midwest, Northeast), event_level = "second")
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(dplyr)
library(readr)
library(broom)
library(ggplot2)
library(splines)
library(tidymodels)
library(gridExtra)
library(maps)
library(caret)
library(vip) #install.packages('vip')
library(probably)
tidymodels_prefer()
COVID_State <- read.csv("COVID - State - Daily.csv", na.strings = ".")
Employment_State <- read.csv("Employment - State - Daily.csv", na.strings = ".")
Mobility_State <- read.csv("Google Mobility - State - Daily.csv", na.strings = ".")
Spending_State <- read.csv("Affinity - State - Daily.csv", na.strings = ".")
regions <- read.csv("regions.csv")
fips <- state.fips
COVID_State$Date<-as.Date(with(COVID_State,paste(year,month,day,sep="-")),"%Y-%m-%d")
Employment_State$Date<-as.Date(with(Employment_State,paste(year,month,day,sep="-")),"%Y-%m-%d")
Mobility_State$Date<-as.Date(with(Mobility_State,paste(year,month,day,sep="-")),"%Y-%m-%d")
Spending_State$Date<-as.Date(with(Spending_State,paste(year,month,day,sep="-")),"%Y-%m-%d")
full_data <- merge(merge(merge(COVID_State, Employment_State, by=c("Date","statefips")), Mobility_State, by=c("Date","statefips")), Spending_State, by=c("Date","statefips"))
head(full_data)
full_data1 <- full_data %>%
select(-year.x, -month.x, -day.x, - year.y, -month.y, -day.y, -year.x )
regions <- regions%>%
inner_join(fips, by=c("State.Code"="abb"))
# Created dataset with the fips code
full_cut <- full_data1 %>%
filter(Date > "2020-04-13")%>%
select(statefips, Date, gps_away_from_home, case_rate, hospitalized_rate, spend_remoteservices, spend_hcs, emp_incbelowmed)%>%
left_join(regions, by=c("statefips"="fips"))
# Final Data Set
full_cut <- full_cut %>%
select(statefips, Date, gps_away_from_home, case_rate, hospitalized_rate, spend_remoteservices, spend_hcs, emp_incbelowmed,State.Code, Region, Division)
# Splitting the Full cut so it has regions for CV
full_cut <- full_cut %>% na.omit()
full_cut <- full_cut %>%
mutate(Region = factor(Region)) %>%
mutate(across(where(is.character), as.factor))
# Re-sampleing
spade_rec <- recipe(Region ~ ., data = full_cut) %>%
step_rm(State.Code, Division, statefips)%>%
step_nzv(all_predictors()) %>%
step_novel(all_nominal_predictors()) %>%
step_dummy(all_nominal_predictors())
# Find out the Total Number of Predictors
(spade_rec %>% prep(full_cut) %>% juice() %>% ncol()) - 1
# Bagging Model Spec
bag_spec <- rand_forest() %>%
set_engine(engine = 'ranger') %>%
set_args(mtry = 61,
trees = 500,
min_n = 20,
probability = FALSE) %>%
set_mode('classification')
region_bag_wf <- workflow() %>%
add_model(bag_spec) %>%
add_recipe(spade_rec)
set.seed(123) # Randomness in the bootstrap samples
region_bag_fit <- region_bag_wf %>%
fit(data = full_cut)
region_bag_fit
region_bag_OOB_output <- tibble(
.pred_class = region_bag_fit %>% extract_fit_engine() %>% pluck('predictions'),
Region = full_cut %>% pull(Region))
bag_metrics <- metric_set(sens, yardstick::spec, accuracy)
region_bag_OOB_output %>%
bag_metrics(truth = Region, estimate = .pred_class)
set.seed(123) # to get the same bootstrap samples, use same seed
region_bag_fit2 <- region_bag_wf %>%
update_model(bag_spec %>% set_args(probability = TRUE)) %>% # Now, we want soft (probability) predictions
fit(data = full_cut)
region_bag_fit2
region_bag_OOB_output2 <- bind_cols(
region_bag_fit2 %>% extract_fit_engine() %>% pluck('predictions') %>% as_tibble(),
full_cut %>% select(Region))
region_bag_OOB_output2 %>%
roc_curve(Region, c(South, West, Midwest, Northeast), event_level = "second") %>% autoplot()
region_bag_OOB_output2 %>%
roc_auc(Region, c(South, West, Midwest, Northeast), event_level = "second") #Area under Curve
rf_spec <- rand_forest() %>%
set_engine(engine = 'ranger') %>%
set_args(mtry = NULL, # size of random subset of variables; default is floor(sqrt(ncol(x)))
trees = 500, # Number of bags
min_n = 20,
probability = FALSE, # want hard predictions first
importance = 'impurity') %>%
set_mode('classification') # change this for regression tree
rf_spec
region_rf_wf <- workflow() %>%
add_model(rf_spec) %>%
add_recipe(spade_rec)
set.seed(123)
region_rf_fit <- region_rf_wf %>%
fit(data = full_cut)
region_rf_fit # check out OOB prediction error (accuracy = 1 - OOB prediction error)
#region_rf_OOB_output <- tibble(
#  .pred_class = region_rf_fit %>% extract_fit_engine() %>% pluck('predictions'),
#  Region = full_cut %>% pull(Region))
#bag_metrics <- metric_set(sens, yardstick::spec, accuracy)
region_rf_OOB_output <- function(fit_model, model_label, truth){
tibble(
.pred_class = region_rf_fit %>% extract_fit_engine() %>% pluck('predictions'), #OOB predictions
Region = truth,
label = model_label
)
}
#check out the function output
region_rf_OOB_output(region_rf_fit2,NULL, full_cut %>% pull(Region))
set.seed(123) #to get the same bootstrap samples, use same seed
region_rf_fit2 <- region_rf_wf %>%
update_model(rf_spec %>% set_args(probability = TRUE)) %>%
fit(data = full_cut)
region_rf_fit2
region_rf_OOB_output2 <- bind_cols(
region_rf_fit2 %>% extract_fit_engine() %>% pluck('predictions') %>% as_tibble(),
full_cut%>% select(Region))
region_rf_OOB_output3 <- bind_cols(
.pred_class = region_rf_fit2 %>% extract_fit_engine() %>% pluck('predictions'),
Region = full_cut %>% pull(Region))
region_rf_OOB_output2 %>%
roc_curve(Region, c(South, West, Midwest, Northeast), event_level = "second") %>% autoplot()
region_rf_OOB_output2 %>%
roc_auc(Region, c(South, West, Midwest, Northeast), event_level = "second") #Area under Curve
region_rf_fit %>% extract_fit_engine() %>% vip() #based on impurity
region_rf_wf %>% #based on permutation
update_model(rf_spec %>% set_args(importance = "permutation")) %>%
fit(data = full_cut) %>% extract_fit_engine() %>% vip()
#rf_OOB_output(data_fit_mtry12,12, land %>% pull(class)) %>%
#    conf_mat(truth = class, estimate= .pred_class)
region_rf_OOB_output(region_rf_fit2, NULL, full_cut %>% pull(Region)) %>%
conf_mat(truth = Region, estimate = .pred_class)
# log_metrics <- metric_set(sens, yardstick::spec, accuracy) # these metrics are based on hard predictions
#sens: sensitivity = chance of correctly predicting second level, given second level (Yes)
#spec: specificity = chance of correctly predicting first level, given first level (No)
#accuracy: accuracy = chance of correctly predicting outcome
# region_rf_OOB_output3 %>%
#   log_metrics(estimate = .pred_class, truth = c(South, West, Midwest, Northeast), event_level = "second")
policy <- read.csv("policy.csv", na.strings = "")
set.seed(253)
policy$date_restrictions_start<-as.Date(policy$date_restrictions_start,"%Y-%m-%d")
policy$date_restrictions_end<-as.Date(policy$date_restrictions_end,"%Y-%m-%d")
policy_cut <- policy %>%
select(statename, statefips, all_restrictions, date_restrictions_start, date_restrictions_end)%>%
filter(statename != "")
full_cut2 <- full_cut%>%
mutate(isSouth = if_else(Region == "South", 1, 0))%>%
inner_join(policy_cut, by=c("statefips"="statefips"))
# Creating a Dummy Variable that is yes for each day in a state where both nonessential buisness closer and stay at home orders were in place..
full_cut2 <- mutate(full_cut2, day_with_allCloser = if_else(Date >= date_restrictions_start & Date <= date_restrictions_end, 1, 0))%>%
filter(Date <= as.Date("2020-06-09","%Y-%m-%d"))
# Log Model and data Cutting
full_cut2$isSouth <- as.factor(full_cut2$isSouth)
full_cut2$day_with_allCloser <- as.factor(full_cut2$day_with_allCloser)
full_cut_sub3 <- full_cut2 %>%
select(gps_away_from_home, case_rate, hospitalized_rate, spend_remoteservices,spend_hcs, emp_incbelowmed, day_with_allCloser)
data_cv10 <- vfold_cv(full_cut_sub3, v = 10)
# Logistic Regression Model Spec
logistic_spec <- logistic_reg() %>%
set_engine('glm') %>%
set_mode('classification')
# Recipe
logistic_rec <- recipe(day_with_allCloser ~ ., data = full_cut_sub3) %>%
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors())
log_wf <- workflow() %>%
add_recipe(logistic_rec) %>%
add_model(logistic_spec)
# Fit Model to Training Data
log_fit <- fit(log_wf, data = full_cut_sub3)
# Print out Coefficients
log_fit %>% tidy()
# Get Exponentiated coefficients + CI
log_fit %>% tidy() %>%
mutate(OR.conf.low = exp(estimate - 1.96*std.error), OR.conf.high = exp(estimate + 1.96*std.error)) %>% # do this first
mutate(OR = exp(estimate))
# Make soft (probability) predictions
predict(log_fit, new_data = full_cut_sub3, type = "prob")
# Make hard (class) predictions (using a default 0.5 probability threshold)
predict(log_fit, new_data = full_cut_sub3, type = "class")
# Soft predictions
logistic_output <-  full_cut_sub3 %>%
bind_cols(predict(log_fit, new_data = full_cut_sub3, type = 'prob'))
# Hard predictions (you pick threshold)
logistic_output <- logistic_output %>%
mutate(.pred_class = make_two_class_pred(.pred_0, levels(day_with_allCloser), threshold = .6))
# Visualize Soft Predictions
logistic_output %>%
ggplot(aes(x = day_with_allCloser, y = .pred_1)) +
geom_boxplot() +
geom_hline(yintercept = 0.6, color='red') +
labs(y = 'Predicted Probability of Outcome', x = 'Observed Outcome') +
theme_classic()
# Confusion Matrix
logistic_output %>%
conf_mat(truth = day_with_allCloser, estimate = .pred_class)
log_metrics <- metric_set(sens, yardstick::spec, accuracy) # these metrics are based on hard predictions
#sens: sensitivity = chance of correctly predicting second level, given second level (Yes)
#spec: specificity = chance of correctly predicting first level, given first level (No)
#accuracy: accuracy = chance of correctly predicting outcome
logistic_output %>%
log_metrics(estimate = .pred_class, truth = day_with_allCloser, event_level = "second") # set second level of outcome as "success"
logistic_roc <- logistic_output %>%
roc_curve(day_with_allCloser, .pred_1, event_level = "second")
autoplot(logistic_roc) + theme_classic()
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(dplyr)
library(readr)
library(broom)
library(ggplot2)
library(splines)
library(tidymodels)
library(gridExtra)
library(maps)
library(caret)
tidymodels_prefer()
COVID_State <- read.csv("COVID - State - Daily.csv", na.strings = ".")
Employment_State <- read.csv("Employment - State - Daily.csv", na.strings = ".")
Mobility_State <- read.csv("Google Mobility - State - Daily.csv", na.strings = ".")
Spending_State <- read.csv("Affinity - State - Daily.csv", na.strings = ".")
regions <- read.csv("regions.csv")
fips <- state.fips
COVID_State$Date<-as.Date(with(COVID_State,paste(year,month,day,sep="-")),"%Y-%m-%d")
Employment_State$Date<-as.Date(with(Employment_State,paste(year,month,day,sep="-")),"%Y-%m-%d")
Mobility_State$Date<-as.Date(with(Mobility_State,paste(year,month,day,sep="-")),"%Y-%m-%d")
Spending_State$Date<-as.Date(with(Spending_State,paste(year,month,day,sep="-")),"%Y-%m-%d")
full_data <- merge(merge(merge(COVID_State, Employment_State, by=c("Date","statefips")), Mobility_State, by=c("Date","statefips")), Spending_State, by=c("Date","statefips"))
head(full_data)
full_data1 <- full_data %>%
select(-year.x, -month.x, -day.x, - year.y, -month.y, -day.y, -year.x )
regions <- regions%>%
inner_join(fips, by=c("State.Code"="abb"))
#created dataset with the fips code
full_cut <- full_data1 %>%
filter(Date > "2020-04-13")%>%
select(statefips, Date, gps_away_from_home, case_rate, hospitalized_rate, spend_remoteservices, spend_hcs, emp_incbelowmed)%>%
left_join(regions, by=c("statefips"="fips"))
full_cut <- full_cut %>%
select(statefips, Date, gps_away_from_home, case_rate, hospitalized_rate, spend_remoteservices, spend_hcs, emp_incbelowmed,State.Code, Region, Division)
full_cut <- full_cut[,-1]
full_cut <- full_cut %>% na.omit() #there are 6 missing values in two variables
full_cut <- full_cut %>%
mutate(Region = factor(Region)) %>% #make sure outcome is factor
mutate(across(where(is.character), as.factor))
set.seed(253)
full_cut <- full_cut %>%
slice_sample(n = 50)
# Select the variables to be used in clustering
full_cut_sub <- full_cut %>%
select(gps_away_from_home, case_rate, hospitalized_rate, spend_remoteservices, spend_hcs, emp_incbelowmed)
# Summary statistics for the variables
summary(full_cut_sub)
# Compute a distance matrix on the scaled data
dist_mat_scaled <- dist(scale(full_cut_sub))
# The (scaled) distance matrix is the input to hclust()
# The method argument indicates the linkage type
hc_complete <- hclust(dist_mat_scaled, method = "complete")
hc_single <- hclust(dist_mat_scaled, method = "single")
hc_average <- hclust(dist_mat_scaled, method = "average")
hc_centroid <- hclust(dist_mat_scaled, method = "centroid")
# Plot dendrograms
plot(hc_complete)
plot(hc_single)
plot(hc_average)
plot(hc_centroid)
#plot with labels
plot(hc_complete, labels = full_cut$Region)
#complete linkage gives tighter, denser clusters because it is easier to split on the 4 clusters that I wanted. I also hoped the clusters would be clearly defined into 4 regions.
#scatterplot with colors
full_cut <- full_cut %>%
mutate(
hclust_height3 = factor(cutree(hc_complete, h = 5)), # Cut at height (h) 3
hclust_num6 = factor(cutree(hc_complete, k = 4)) # Cut into 4 clusters (k)
)
ggplot(full_cut, aes(x=gps_away_from_home, y=case_rate, color=hclust_height3))+
geom_point()+
theme_bw()
# Look at summary statistics of the 3 variables
full_cut_cut <- full_cut %>%
select(gps_away_from_home, case_rate, hospitalized_rate, spend_remoteservices,spend_hcs, emp_incbelowmed)
summary(full_cut_cut)
# Perform clustering: should you use scale()?
set.seed(253)
kclust_k3_3vars <- kmeans(scale(full_cut_cut), centers = 4)
full_cut_sub2 <- full_cut %>%
mutate(kclust_3_3vars = factor(kclust_k3_3vars$cluster))
#can type out more variables if you want to see
full_cut_sub2 %>%
group_by(kclust_3_3vars) %>%
summarize(across(c(gps_away_from_home, case_rate, spend_remoteservices,spend_hcs, emp_incbelowmed), mean))
#vizualizing two random variables
ggplot(full_cut_sub2, aes(y=gps_away_from_home, x=case_rate, color=kclust_3_3vars, shape=Region)) +
geom_point()
#confusion matrix table
calculate_mode <- function(x) {
uniqx <- unique(na.omit(x))
uniqx[which.max(tabulate(match(x, uniqx)))]
}
confMatrix_k <- full_cut_sub2 %>%
select(Region, kclust_3_3vars)%>%
group_by(Region)%>%
summarise(cluster=as.numeric(calculate_mode(kclust_3_3vars)))
confMatrix_k
confMatrix_k[confMatrix_k$Region=="West", "cluster"] <- 2
#full_cut_sub2 <- full_cut_sub2 %>%
#  mutate(regionNum = as.numeric(case_when(Region=="Midwest"~3, #Region=="Northeast"~4,Region=="South"~1,Region=="West"~2)))
full_cut_sub2 <- full_cut_sub2 %>%
mutate(Pred.Region = as.factor(case_when(kclust_3_3vars==3~"Midwest", kclust_3_3vars==4~"Northeast",kclust_3_3vars==1~"South",kclust_3_3vars==2~"West")))
print(confMatrix_k)
full_cut_sub2 %>%
conf_mat(truth = Region, estimate = Pred.Region)
log_metrics <- metric_set(sens, yardstick::spec, accuracy)
full_cut_sub2 %>%
log_metrics(estimate = Pred.Region, truth = Region)
rand_index <- adj.rand.index(full_cut_sub2$Region, full_cut_sub2$Pred.Region)
full_cut_sub2 %>%
conf_mat(truth = Region, estimate = Pred.Region)
log_metrics <- metric_set(sens, yardstick::spec, accuracy)
full_cut_sub2 %>%
log_metrics(estimate = Pred.Region, truth = Region)
rand_index <- adj.rand.index(full_cut_sub2$Region, full_cut_sub2$Pred.Region)
full_cut_sub2 %>%
conf_mat(truth = Region, estimate = Pred.Region)
log_metrics <- metric_set(sens, yardstick::spec, accuracy)
full_cut_sub2 %>%
log_metrics(estimate = Pred.Region, truth = Region)
rand_index <- adj.rand.index(full_cut_sub2$Region, full_cut_sub2$Pred.Region)
#confusion matrix table
calculate_mode <- function(x) {
uniqx <- unique(na.omit(x))
uniqx[which.max(tabulate(match(x, uniqx)))]
}
confMatrix_k <- full_cut_sub2 %>%
select(Region, kclust_3_3vars)%>%
group_by(Region)%>%
summarise(cluster=as.numeric(calculate_mode(kclust_3_3vars)))
confMatrix_k
confMatrix_k[confMatrix_k$Region=="West", "cluster"] <- 2
#full_cut_sub2 <- full_cut_sub2 %>%
#  mutate(regionNum = as.numeric(case_when(Region=="Midwest"~3, #Region=="Northeast"~4,Region=="South"~1,Region=="West"~2)))
full_cut_sub2 <- full_cut_sub2 %>%
mutate(Pred.Region = as.factor(case_when(kclust_3_3vars==3~"Midwest", kclust_3_3vars==4~"Northeast",kclust_3_3vars==1~"South",kclust_3_3vars==2~"West")))
print(confMatrix_k)
full_cut_sub2 %>%
conf_mat(truth = Region, estimate = Pred.Region)
log_metrics <- metric_set(sens, yardstick::spec, accuracy)
full_cut_sub2 %>%
log_metrics(estimate = Pred.Region, truth = Region)
rand_index <- adj.rand.index(full_cut_sub2$Region, full_cut_sub2$Pred.Region)
install.packages('pdfCluster')
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(dplyr)
library(readr)
library(broom)
library(ggplot2)
library(splines)
library(tidymodels)
library(gridExtra)
library(maps)
library(caret)
tidymodels_prefer()
COVID_State <- read.csv("COVID - State - Daily.csv", na.strings = ".")
Employment_State <- read.csv("Employment - State - Daily.csv", na.strings = ".")
Mobility_State <- read.csv("Google Mobility - State - Daily.csv", na.strings = ".")
Spending_State <- read.csv("Affinity - State - Daily.csv", na.strings = ".")
regions <- read.csv("regions.csv")
fips <- state.fips
COVID_State$Date<-as.Date(with(COVID_State,paste(year,month,day,sep="-")),"%Y-%m-%d")
Employment_State$Date<-as.Date(with(Employment_State,paste(year,month,day,sep="-")),"%Y-%m-%d")
Mobility_State$Date<-as.Date(with(Mobility_State,paste(year,month,day,sep="-")),"%Y-%m-%d")
Spending_State$Date<-as.Date(with(Spending_State,paste(year,month,day,sep="-")),"%Y-%m-%d")
full_data <- merge(merge(merge(COVID_State, Employment_State, by=c("Date","statefips")), Mobility_State, by=c("Date","statefips")), Spending_State, by=c("Date","statefips"))
head(full_data)
full_data1 <- full_data %>%
select(-year.x, -month.x, -day.x, - year.y, -month.y, -day.y, -year.x )
regions <- regions%>%
inner_join(fips, by=c("State.Code"="abb"))
#created dataset with the fips code
full_cut <- full_data1 %>%
filter(Date > "2020-04-13")%>%
select(statefips, Date, gps_away_from_home, case_rate, hospitalized_rate, spend_remoteservices, spend_hcs, emp_incbelowmed)%>%
left_join(regions, by=c("statefips"="fips"))
full_cut <- full_cut %>%
select(statefips, Date, gps_away_from_home, case_rate, hospitalized_rate, spend_remoteservices, spend_hcs, emp_incbelowmed,State.Code, Region, Division)
full_cut <- full_cut[,-1]
full_cut <- full_cut %>% na.omit() #there are 6 missing values in two variables
full_cut <- full_cut %>%
mutate(Region = factor(Region)) %>% #make sure outcome is factor
mutate(across(where(is.character), as.factor))
set.seed(253)
full_cut <- full_cut %>%
slice_sample(n = 50)
# Select the variables to be used in clustering
full_cut_sub <- full_cut %>%
select(gps_away_from_home, case_rate, hospitalized_rate, spend_remoteservices, spend_hcs, emp_incbelowmed)
# Summary statistics for the variables
summary(full_cut_sub)
# Compute a distance matrix on the scaled data
dist_mat_scaled <- dist(scale(full_cut_sub))
# The (scaled) distance matrix is the input to hclust()
# The method argument indicates the linkage type
hc_complete <- hclust(dist_mat_scaled, method = "complete")
hc_single <- hclust(dist_mat_scaled, method = "single")
hc_average <- hclust(dist_mat_scaled, method = "average")
hc_centroid <- hclust(dist_mat_scaled, method = "centroid")
# Plot dendrograms
plot(hc_complete)
plot(hc_single)
plot(hc_average)
plot(hc_centroid)
#plot with labels
plot(hc_complete, labels = full_cut$Region)
#complete linkage gives tighter, denser clusters because it is easier to split on the 4 clusters that I wanted. I also hoped the clusters would be clearly defined into 4 regions.
#scatterplot with colors
full_cut <- full_cut %>%
mutate(
hclust_height3 = factor(cutree(hc_complete, h = 5)), # Cut at height (h) 3
hclust_num6 = factor(cutree(hc_complete, k = 4)) # Cut into 4 clusters (k)
)
ggplot(full_cut, aes(x=gps_away_from_home, y=case_rate, color=hclust_height3))+
geom_point()+
theme_bw()
# Look at summary statistics of the 3 variables
full_cut_cut <- full_cut %>%
select(gps_away_from_home, case_rate, hospitalized_rate, spend_remoteservices,spend_hcs, emp_incbelowmed)
summary(full_cut_cut)
# Perform clustering: should you use scale()?
set.seed(253)
kclust_k3_3vars <- kmeans(scale(full_cut_cut), centers = 4)
full_cut_sub2 <- full_cut %>%
mutate(kclust_3_3vars = factor(kclust_k3_3vars$cluster))
#can type out more variables if you want to see
full_cut_sub2 %>%
group_by(kclust_3_3vars) %>%
summarize(across(c(gps_away_from_home, case_rate, spend_remoteservices,spend_hcs, emp_incbelowmed), mean))
#vizualizing two random variables
ggplot(full_cut_sub2, aes(y=gps_away_from_home, x=case_rate, color=kclust_3_3vars, shape=Region)) +
geom_point()
#confusion matrix table
calculate_mode <- function(x) {
uniqx <- unique(na.omit(x))
uniqx[which.max(tabulate(match(x, uniqx)))]
}
confMatrix_k <- full_cut_sub2 %>%
select(Region, kclust_3_3vars)%>%
group_by(Region)%>%
summarise(cluster=as.numeric(calculate_mode(kclust_3_3vars)))
confMatrix_k
confMatrix_k[confMatrix_k$Region=="West", "cluster"] <- 2
#full_cut_sub2 <- full_cut_sub2 %>%
#  mutate(regionNum = as.numeric(case_when(Region=="Midwest"~3, #Region=="Northeast"~4,Region=="South"~1,Region=="West"~2)))
full_cut_sub2 <- full_cut_sub2 %>%
mutate(Pred.Region = as.factor(case_when(kclust_3_3vars==3~"Midwest", kclust_3_3vars==4~"Northeast",kclust_3_3vars==1~"South",kclust_3_3vars==2~"West")))
print(confMatrix_k)
full_cut_sub2 %>%
conf_mat(truth = Region, estimate = Pred.Region)
log_metrics <- metric_set(sens, yardstick::spec, accuracy)
full_cut_sub2 %>%
log_metrics(estimate = Pred.Region, truth = Region)
rand_index <- adj.rand.index(full_cut_sub2$Region, full_cut_sub2$Pred.Region)
install.packages(pdfCluster)
install.packages('pdfcluster')
y
\
install.packages('pdfCluster')
log_metrics <- metric_set(sens, yardstick::spec, accuracy)
full_cut_sub2 %>%
log_metrics(estimate = Pred.Region, truth = Region)
rand_index <- adj.rand.index(full_cut_sub2$Region, full_cut_sub2$Pred.Region)
